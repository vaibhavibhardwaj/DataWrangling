{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT5196-S2-2020 assessment 1\n",
    "# Task 1: Parsing Text Files (%55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents & Steps for the Task\n",
    "* [A. Introduction](#A.-Introduction)\n",
    "* [B. Methodology](#B.-Methodology)\n",
    "    * [1. Importing Libraries](#1.-Importing-Libraries)\n",
    "    * [2. Reading Files From 30154987 Folder](#2.-Reading-files-from-30154987-folder)\n",
    "        * [2.1 Total Files](#2.1-Total-Files)\n",
    "    * [ 3. Retriving Data And Converting To Desiarable Format.](#3.-Retriving-Data-And-Converting-To-Desiarable-Format.)\n",
    "        * [3.1 Total Dates](#3.1-Total-Dates)\n",
    "    * [4. Writing to XML file.](#4.-Writing-to-XML-file.)\n",
    "        * [4.1 Total Tweets : Conclusion of Step 4](#4.1-Total-Tweets-:-Conclusion-of-Step-4)\n",
    "* [C. Conclusion](#C.-Conclusion)\n",
    "* [D. References](#D.-References)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task involves using regex for finding the relevant files and langid to find the tweets which are in English. The task started with importing the libraries and later parsing the files with respect to the type of content in the files. Furthermore, the tweets have emojis and special characters that are supposed to be considered before examining the tweets for further analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task was performed only by 3 importing libraries.\n",
    "Usage of specified libraries:\n",
    "   - **import os** : for dealing with files and path name so that easier retrival of files is possible\n",
    "   - **import re** : for implementing regex while finding date in a desired format.\n",
    "   - **import langid** : used to classify what language a tweet is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-1  ##############################################################\n",
    "\n",
    "# Importing recommended for the assignment\n",
    "import os\n",
    "import re\n",
    "import langid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading Files From 30154987 Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#######################################################   STEP-2   #############################################################\n",
    "\n",
    "# Loaction of the twitter files \n",
    "path = \"30154987/\"\n",
    "# all the files in a list\n",
    "all_files = os.listdir(path)\n",
    "\n",
    "# list initialised to contain all the files as string\n",
    "# each index of the list has one file \n",
    "file_list = list()\n",
    "\n",
    "# reading the files by running a loop over all_files \n",
    "for file_name in all_files:\n",
    "   # open the file and then call .read() to get the text\n",
    "   with open(os.path.join(path, file_name), encoding='utf-8') as f:\n",
    "        # reading the file\n",
    "        file = f.read()\n",
    "        # adding the file string to the list\n",
    "        file_list.append(file)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Total Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On printing the files it can be seen that the files are in json format and can be used further to be converted as a dictionary as key-value pair are present in the given text files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_list) # gives output as 2416 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Retriving Data And Converting To Desiarable Format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Substeps:\n",
    "   - **Step 3.A** : File list made from the jason txt files evalusted as a dictionary by using eval()\n",
    "   \n",
    "   - **Step 3.B** : The resultant dictionary goes through data cleaning as english tweets are taken into consideration for this assignment. The created at coloumn goes under regex to get only the date from the time stamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **Step 3.C** : The resultant cleaned dictionary is passed on to a parent dummy dictionary which has all the english tweets with respective tweet id and created at date as the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-3   #############################################################\n",
    "\n",
    "# making a temporary dictionary to store id, text and created_at values of the tweets\n",
    "dummy_dict = dict()\n",
    "\n",
    "# running a loop on the file_list\n",
    "for i in range(len(file_list)):\n",
    "    \n",
    "    #false = False # NameError: name 'false' is not defined\n",
    "    #true = True #NameError: name 'true' is not defined\n",
    "    global false, null, true\n",
    "    false = null = true = '' # defined to overcome errors\n",
    "    \n",
    "    # Step 3.A\n",
    "    # making a dictionary out the file string\n",
    "    # as the jason file given to us as input is a dictionary\n",
    "    # using eval will give us a dictionary of the jason file which is converted to a string in file_list at index i\n",
    "    file_text = eval(file_list[i]) \n",
    "   \n",
    "    \n",
    "    # file_text contains keys as \"data\" and value as list of dictionary  with \"id\",\"created_at\" and \"text\" as the key.\n",
    "    # running a loop across the number of tweets in \"data\" which are lists of dictionaries\n",
    "    for j in range(len(file_text[\"data\"])):\n",
    "        \n",
    "        # Step 3.B\n",
    "        # Reading the date from the created_at field of the dictionary\n",
    "        # and using regex to get it in the specified form\n",
    "        date_check = re.findall(r\"(\\d{4}\\-\\d{2}\\-\\d{2})\",file_text[\"data\"][j][\"created_at\"])[0]\n",
    "        \n",
    "        #geting the id from the file_text dictionary of key \"data\" at ith element of the list\n",
    "        id_check = file_text[\"data\"][j][\"id\"]\n",
    "        \n",
    "        # retrievinf text form the filr_text dictionary of key \"data\" at ith element of the list\n",
    "        # encoding and decoding the retrived text so that langid can work properly and can also read emojis.\n",
    "        tweet_check = file_text[\"data\"][j][\"text\"].encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "        \n",
    "        # Step 3.C\n",
    "        # checking of the text_check is english or not\n",
    "        # as we need to take englisg tweets into consideration for this assignment\n",
    "        if langid.classify(tweet_check)[0]=='en': # taking english tweets  further to be included in the dummy dictionary\n",
    "                \n",
    "                # checking if date_check is a key or not\n",
    "                if date_check in dummy_dict:                    \n",
    "                    # making id_check as the key for tweet_check\n",
    "                    # date_check is the parent key for id_check ( nested dictionary)\n",
    "                    dummy_dict[date_check][id_check]=tweet_check\n",
    "                    \n",
    "                else:\n",
    "                    # making a dictionary with date_check as the key\n",
    "                    dummy_dict[date_check] = {}\n",
    "                    dummy_dict[date_check][id_check]=tweet_check\n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Total Days : Conclusion of Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The file final dummy_dict has all the tweets with tweet date and id as the keys and text as the value.\n",
    "By evaluating the lenght of dummy_dict we can see that **81 distinct dates** were found from the given data in 30154987 folder.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dummy_dict) # gives output as 81 days "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Writing to XML file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resultant dummy_dict can be used to write in the XML file given the desired format in the sample output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-4   #############################################################\n",
    "\n",
    "tweet_count = 0 # variable assigned to count the total number of desirable tweets \n",
    "\n",
    "# writing out output in the xml file\n",
    "with open('30154987.xml', 'w', encoding='utf-8') as f:\n",
    "    # writing into the xml file for the desired format as given in the sample output\n",
    "    f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    f.write('<data>\\n')\n",
    "    \n",
    "    # iterating on the dummy_dict to get all tweets with \n",
    "    for i in range(len(dummy_dict)):\n",
    "        \n",
    "        # writing date from list(dummy_dict.keys())\n",
    "        # as the keys in dummy_dict is tweet date\n",
    "        f.write('<tweets date=\"'+list(dummy_dict.keys())[i]+'\">\\n')\n",
    "        \n",
    "        # getting the id values for a particular date from dummy_dict with list(date dummy_dict.keys())[i]\n",
    "        # j is the id and the keys of dummy_dict[list(dummy_dict.keys())[i]].keys()\n",
    "        for j in list(dummy_dict[list(dummy_dict.keys())[i]].keys()):\n",
    "            tweet_count = tweet_count + 1 # counting the tweets \n",
    "            \n",
    "            # printing the tweet id and tweet text \n",
    "            # getting the text with keys as [list(dummy_dict.keys())[i]][j] for dummy_dict\n",
    "            f.write('<tweet id=\"' + j + '\">' + dummy_dict[list(dummy_dict.keys())[i]][j]\\\n",
    "                    .encode('utf-16', 'surrogatepass').decode('utf-16') + '</tweet>\\n')\n",
    "            \n",
    "        # writing along the given format of the sample output\n",
    "        # ending the data for that parricular date\n",
    "        f.write('</tweets>\\n')\n",
    "    \n",
    "    # closing the data tag after writing all the tweets in the xml file\n",
    "    f.write('</data>')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Total Tweets : Conclusion of Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While writing in the file it's clear that the number of tweets from the original counts have reduced drasticaly as duplicates and non english tweets have reduced. We can see that tweet_count has value **120471** which is the total number of desirable tweets in **81** days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_count # gives output as 120471 tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task presented a challenge when it comes to dealing with emojis and removing non-English tweets with special characters.  Finally, it can be seen that 120471 are the relevant number of tweets for 81 days that are only English and are unique in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tutorials form Week 1 to 6. (FIT5196)\n",
    "- Lecture resources form week 1 to 6.(FIT5196)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
