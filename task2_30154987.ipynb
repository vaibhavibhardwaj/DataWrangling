{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT5196-S2-2020 assessment 1\n",
    "# Task 2: Text Pre-Processing (%45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "* [A. Introduction](#A.-Introduction)\n",
    "* [B. Steps for the Task (METHODOLOGY)](#B.-Steps-for-the-Task-(METHODOLOGY))\n",
    "* [C. Conclusion](#C.-Conclusion)\n",
    "* [D. References](#D.-References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2,  the way I have done is done by following the below-given steps in the said order. The steps start by Importing the files, Reading the excel sheets, retrieving and converting the data into a desirable format, after which tokenization is done. It can be seen after tokenization that Unigram 100 and Bigram 100 files can be easily created just by stop word removal and stemming for unigram 100 files and by using ngram get can get bigram 100 output file.\n",
    "\n",
    "When it comes to Vocabulary building and countVec output files, I have followed a different procedure to complete the task. I started by making the 200 bigram list and later on added it to the unigram token dictionary where by using MWETokenizer the bigrams were created if present, in that particular dictionary key-value pair. After that, I drop the records with lengths less than 3 and split my dictionary into unigram and bigram dictionary. On the newly formed unigram dictionary, context-dependent words and rare words were removed by using FreqDist, and later on, stemming is performed. \n",
    "\n",
    "After the above steps, the bigram dictionary and the minimized unigram dictionary are combined into one. The newly formed dictionary is the final dictionary containing only the words desirable in the vocab. Finally, the vocabulary is formed by the dictionary values and sorted to be written in the vocab output file.  Furthermore, a vector is formed by CountVectorizer to be\n",
    "to be written in the countVec file.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Steps for the Task (METHODOLOGY)\n",
    "* [1. Importing Libraries](#1.-Importing-Libraries)\n",
    "* [2. Reading Excel Data](#2.-Reading-Excel-Data)\n",
    "\t* [2.1 Total Files](#2.1-Total-Files)\n",
    "* [ 3. Retriving Data And Converting To Desiarable Format.](#3.-Retriving-Data-And-Converting-To-Desiarable-Format.)\n",
    "\t* [3.1 Drop Counts](#3.1-Drop-Counts)\n",
    "* [4. Tokenisation Using RegexTokenizer](#4.-Tokenisation-Using-RegexTokenizer)\n",
    "* [5.Working for 100 UNI/BIGRAM File](#5.-Working-for-100-UNI/BIGRAM-File)\n",
    "    * [5.1 Output File : 100 BIGRAM](#5.1-Output-File-:-100-BIGRAM)\n",
    "    * [5.2 Removing-Context-Independent-words](#5.2-Removing-Context-Independent-words)\n",
    "    * [5.3 Stemming](#5.3-Stemming)\n",
    "        * [5.3.1 Output File : 100 UNIGRAM](#5.3.1-Output-File-:-100-UNIGRAM)\n",
    "* [6. 200 Bi-grams for Vocab](#6.-200-Bi-grams-for-Vocab)\n",
    "* [7. Vocab Building](#7.-Vocab-Building)\n",
    "\t* [7.1 Adding the 200 bigrams to each Dictionary value](#7.1-Adding-the-200-bigrams-to-each-Dictionary-value)\n",
    "\t* [7.2 Running MWE for bigram tokenisation.](#7.2-Running-MWE-for-bigram-tokenisation.)\n",
    "\t* [7.3 Removing words of length less than 3.](#7.3-Removing-words-of-length-less-than-3.)\n",
    "\t* [7.4 Splitting Dictionary into bigram and unigram](#7.4-Splitting-Dictionary-into-bigram-and-unigram)\n",
    "    * [7.5 Context Dependent and Rare word removal](#7.5-Context-Dependent-and-Rare-word-removal )\n",
    "    * [7.6 Stemming of the unigram dictionary](#7.6-Stemming-of-the-unigram-dictionary)\n",
    "    * [7.7 Making a final dictionary with unigrams and Bigrams Combined](#7.7-Making-a-final-dictionary-with-unigrams-and-Bigrams-Combined)\n",
    "    * [7.8 Output File : Vocab File](#7.8-Output-File-:-Vocab-File)\n",
    "    * [7.9 Output File : countVec file using CountVectorizer](#7.9-Output-File-:-countVec-file-using-CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-1  ##############################################################\n",
    "import nltk\n",
    "import xlrd\n",
    "import langid\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading Excel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-2  ##############################################################\n",
    "\n",
    "# Reading 30154987.xlsx data\n",
    "\n",
    "# reading data into a dataframe\n",
    "excel_data2 = pd.ExcelFile(\"30154987.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throws 81 as the output means 81 files or 81 days \n",
    "len(excel_data2.sheet_names) # throws 81 as the output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Retriving Data And Converting To Desiarable Format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps in Step 3**\n",
    "   - **Step 3.a** : Reading the data in the excel data one sheet at a time. Each sheet is read into a datafarme. \n",
    "   - **Step 3.b**: The dataframe is cleansed of null values and restructured to get the column name as id,text and created_at\n",
    "   - **Step 3.c** : All the tweets in the dataframe is check if they are english or not and non eglish onces are dropped. \n",
    "   - **Step 3.d** : Duplicate tweet are dropped from the dataframe\n",
    "   - **Step 3.e** : Finaly the dictionary is made with the cleared dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-3  ##############################################################\n",
    "\n",
    "# This is a dictionary initialised to get dictionary of unique tweets for further process.\n",
    "data2 = {}\n",
    "\n",
    "# tweet for checking duplicates\n",
    "pre_english, pre = 0,0 # count before removing duplicates\n",
    "post_english, post = 0,0 # count after removing duplicates\n",
    "\n",
    "\n",
    "# itarating through the excel_data2.sheet_names\n",
    "# getting the excel sheet index as i \n",
    "for i in range(len(excel_data2.sheet_names)):\n",
    "    \n",
    "    \n",
    "    # temporary dataframe formed to input data from each excel sheet and after language analysis it is dropped into data2\n",
    "    # taking the header as None \n",
    "    dfn = excel_data2.parse(i,header=None)\n",
    "    \n",
    "    # dropping all vacant rows and columns \n",
    "    dfn = dfn.dropna(axis='columns',how='all').dropna(axis='rows',how='all')\n",
    "    # reseting index after removing null rows and columns \n",
    "    dfn = dfn.reset_index(drop=True)\n",
    "    \n",
    "    # taking the header of the dataframe which has id,text and created_at as the keys\n",
    "    header = dfn.iloc[0]\n",
    "    \n",
    "    # dropping the header row \n",
    "    # and assigning the header as the column names\n",
    "    dfn = dfn[1:]\n",
    "    dfn = dfn.rename(columns = header)\n",
    "    dfn = dfn.reset_index(drop=True)\n",
    "    \n",
    "    # list made to store the index of null non english tweets\n",
    "    non_english_tweet_index = list()\n",
    "    \n",
    "    pre_english = pre_english + len(dfn)# tweet counts before non english removal\n",
    "    \n",
    "    # running a loop through the rows of the dataframe to find non english tweets\n",
    "    for index in range(len(dfn.text)):\n",
    "        \n",
    "        # checking the instance of the tweets if int than append in non english list\n",
    "        if isinstance(dfn.text[index],int)==True:\n",
    "            non_english_tweet_index.append(index)\n",
    "        \n",
    "        # checking the language \n",
    "        elif langid.classify(dfn.text[index])[0]!=\"en\":\n",
    "            # non english appened to the list\n",
    "            non_english_tweet_index.append(index)\n",
    "     \n",
    "    \n",
    "    #dropping non english tweets\n",
    "    dfn = dfn.drop(non_english_tweet_index)\n",
    "    \n",
    "    \n",
    "    post_english = post_english+ len(dfn) # tweet counts after non english removal\n",
    "    pre = pre + len(dfn) # tweet counts before duplicate removal\n",
    "    \n",
    "    # sorting data by id and dropping duplicate tweets\n",
    "    dfn = dfn.sort_values('id', ascending=False)\n",
    "    dfn = dfn.drop_duplicates(subset='id', keep='first')  \n",
    "   \n",
    "    post = post + len(dfn) # tweet counts after duplicate removal \n",
    "    \n",
    "    #finaly adding the dataframe to the dictionary data2\n",
    "    data2[excel_data2.sheet_names[i]]= '\\n'.join(dfn.text.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Drop Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be clearly seen that the drops are high as we remove non english and and duplicate tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pre_english,\"tweet counts before non english removal\")\n",
    "print(post_english,  \"tweet counts after non english removal\") \n",
    "print(pre,\"tweet counts before duplicate removal\")\n",
    "print(post,\"tweet counts after duplicate removal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *162000 tweet counts before non english removal*\n",
    "- *91991 tweet counts after non english removal*\n",
    "- *91991 tweet counts before duplicate removal*\n",
    "- *91721 tweet counts after duplicate removal*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tokenisation Using RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-4  ##############################################################\n",
    "\n",
    "#  Buiilding the tokenisation function for the data\n",
    "def tokenizeRawData(fileid): \n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "    token = tokenizer.tokenize(data2[fileid].lower())\n",
    "    return (fileid, token)\n",
    "\n",
    "# calling tokenizeRawData to the tokenise data2 to form a dictionary named tokenized_data2.\n",
    "tokenized_data2 = dict(tokenizeRawData(fileid) for fileid in data2.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Working for 100 UNI/BIGRAM File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formation of 100 Bigram file is done by performing the following tasks in order keeping in respect the output from above steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Output File : 100 BIGRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams are directly made using ngrams from the tokenised dictionary.\n",
    "After tokensation it is recommended to make a bigram file because if we do this after context or stop word removal it will lead to loss of records where we fail to find the most common bigrams for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-5  ##############################################################\n",
    "\n",
    "# 100 bigram file <student_number>_100bi\n",
    "#tokenized_data2  tokensied dict\n",
    "from nltk.util import ngrams\n",
    "with open('30154987_100bi.txt', 'w', encoding='utf-8') as f:\n",
    "    for fileid in data2.keys():\n",
    "        bigrams = ngrams(tokenized_data2[fileid], n = 2)\n",
    "        fdbigram = FreqDist(bigrams)\n",
    "        f.write(fileid+':'+ str(fdbigram.most_common(100))+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Removing Context-Independent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Unigram 100 file context-independent words are removed because taking them would lead to unnecessary stop word counts which will overcome other meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the stop words from the given txt file and adding it to a list\n",
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing Stop words from the dictionary and assiging it to a different dictionary\n",
    "tokenized_data2_1 = {}\n",
    "for fileid in data2.keys():\n",
    "    tokenized_data2_1[fileid] = [w for w in tokenized_data2[fileid] if w not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is performed using Porter Stemmer in this assignment for unigram creation as it's necessary for unigrams to be stemmed to reduce words with similar meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming without context dependent removal\n",
    "# unigram list 100 \n",
    "tokenized_data2_14 = {}\n",
    "for fileid in data2.keys():\n",
    "    tokenized_data2_14[fileid] = [stem.stem(w) for w in tokenized_data2_1[fileid] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.1 Output File : 100 UNIGRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 100 unigram files are written into the output by the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 unigrams in each file \n",
    "# <student_number>_100uni\n",
    "from nltk.probability import *\n",
    "# writing in the file \n",
    "with open('30154987_100uni.txt', 'w', encoding='utf-8') as f:\n",
    "    for fileid in data2.keys():\n",
    "        # taking the most common top 100 freq dist from each file \n",
    "        f.write(fileid+':'+ str(FreqDist(tokenized_data2_14[fileid]).most_common(100))+\"\\n\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 200 Bi-grams for Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is specified that 200 Bigrams should be included in the vocab and to be made by pmi collocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-6  ##############################################################\n",
    "\n",
    "## for 200 bi-gram\n",
    "all_words_tokenized_data = list(chain.from_iterable(tokenized_data2.values()))\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(all_words_tokenized_data)\n",
    "bigram_list_data = finder.nbest(bigram_measures.pmi, 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Vocab Building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Adding the 200 bigrams to each Dictionary value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make add the bigrams to the unigram dictionary because we want to prepare our dictionary for MWETokenizer and find if bigrams are present in the dictionary value for a particular key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-7.1  ############################################################\n",
    "\n",
    "# This step is performed to get the unigrams from each file for running it in the MWE tokenizer.\n",
    "tokenized_data2_122 = {}\n",
    "for fileid in data2.keys():\n",
    "    # making a set of each key value pair  \n",
    "    tokenized_data2_122[fileid] = list(set(tokenized_data2_1[fileid]))\n",
    "    # now adding all the values from bigram_list_data to the dictionary \n",
    "    tokenized_data2_122[fileid] = tokenized_data2_122[fileid] + bigram_list_data\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Running MWE for bigram tokenisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the bigrams present and converting them to tokens which are joined with underscore is what MWETokenizer does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-7.2  ############################################################\n",
    "\n",
    "tokenized_data2_122_MWE = {}\n",
    "\n",
    "from nltk.tokenize import MWETokenizer\n",
    "### write your code below\n",
    "for fileid in data2.keys():\n",
    "    mwe_tokenizer = MWETokenizer(tokenized_data2_122[fileid]) # passing dictionary value with bigrams and unigram set\n",
    "    tokenized_data2_122_MWE[fileid] = mwe_tokenizer.tokenize(tokenized_data2_1[fileid]) # passing dictionary to be tokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenized_data2_122_MWE is the dictionary with bigram and unigram tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.3 Removing words of length less than 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as the above-created dictionary has both unigrams and bigrams it's clear that we can now remove token with lengths less than 3. The reason it is done now is because stemming being done later on. Stemming is always done after removing irrelevant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-7.3  ############################################################\n",
    "\n",
    "# removing words  than 3 length\n",
    "tokenized_data2_more_than3 ={}\n",
    "\n",
    "for fileid in data2.keys():\n",
    "    tokenized_data2_more_than3[fileid] = [w for w in tokenized_data2_122_MWE[fileid] if len(w)>=3]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.4 Splitting Dictionary into bigram and unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting of the dictionary is done because context dependent and rare words ar to be removed from the unigrams. \n",
    "2 dictionaries are made with unigrams and bigrams in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-7.4  ############################################################\n",
    "\n",
    "# Spliting dictionary into 2 parts dictionaries \n",
    "bigram_dict_2 ={} # dictionary containing bigrams\n",
    "non_bigram_dict ={} # dictionary containing unigrams\n",
    "\n",
    "for fileid in data2.keys():\n",
    "    bigram_dict_2[fileid]= [w for w in tokenized_data2_more_than3[fileid] if \"_\" in w] # if _ than a bigram\n",
    "    non_bigram_dict[fileid] =  [w for w in tokenized_data2_more_than3[fileid] if \"_\" not in w]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.5 Context Dependent and Rare word removal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From unigram dictionary rare words and context dependent words are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-7.5  ############################################################\n",
    "\n",
    "# Removal from Unigram Dictionary\n",
    "# making a list of all words to make a documnet frequency\n",
    "non_clean_word_list = list(chain.from_iterable([set(value) for value in non_bigram_dict.values()]))\n",
    "\n",
    "# Getting the document frequency of each unigram\n",
    "fd_of_all_words = FreqDist(non_clean_word_list)\n",
    "df_list_words_data = fd_of_all_words.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code finds the rare words and context dependent words in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# making of list of words which are less than 5 or greater than 60\n",
    "words_context_dependent = list()\n",
    "\n",
    "# iterating through the list df_list_words_data\n",
    "for i in range(len(df_list_words_data)):\n",
    "    # df_list_words_data[i][1] gives the document freq of the word\n",
    "    if df_list_words_data[i][1] <5 or df_list_words_data[i][1] >60:\n",
    "        # append in the list\n",
    "        words_context_dependent.append(df_list_words_data[i][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list into set for faster processing \n",
    "words_context_dependent_set = set(words_context_dependent)\n",
    "\n",
    "# making of dictionary with uni gram which are more than length 3 \n",
    "#and have document frequency more than 5 or less than 60\n",
    "uni_non_context_independent = {}\n",
    "for fileid in data2.keys():\n",
    "    # removing words not in words_context_dependent_set\n",
    "    uni_non_context_independent[fileid] = [w for w in non_bigram_dict[fileid] if w not in words_context_dependent_set]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.6 Stemming of the unigram dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we proceed with unigram stemming to find the relevant unigrams for the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-7.6  ############################################################\n",
    "\n",
    "# stemming the file uni gram dictinary using porter stemmer \n",
    "# stemming should only be done for unigrams\n",
    "stemmed_uni_grams_final = {}\n",
    "\n",
    "for fileid in data2.keys():\n",
    "    # storing the stemmed value in the dictinary\n",
    "    stemmed_uni_grams_final[fileid] = [stem.stem(w) for w in uni_non_context_independent[fileid]]\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.7 Making a final dictionary with unigrams and Bigrams Combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, we can finally make our dictionary containing relevant vocabs for unigrams and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-7.7  ############################################################\n",
    "\n",
    "# the tokens in the dictionary are from unigrams and bigrams\n",
    "# all tokens are present in the vocab\n",
    "final_uni_bi_dict2 = {}\n",
    "\n",
    "for i in data2.keys():\n",
    "    # taking the values of unigram dictinary\n",
    "    final_uni_bi_dict2[i]=[w for w in stemmed_uni_grams_final[i]] \n",
    "    # taking the values from bigram dictionary\n",
    "    final_uni_bi_dict2[i] =final_uni_bi_dict2[i]+  bigram_dict_2[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the values of the final dictionary to form  a vocab\n",
    "final_uni_bi_dict_list = list(chain.from_iterable([set(value) for value in final_uni_bi_dict2.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a sorted list to be used for printing in the vocab file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a sorted list out of the vocab list\n",
    "final_uni_bi_dict_set = set(final_uni_bi_dict_list)\n",
    "final_uni_bi_dict_set = sorted(final_uni_bi_dict_set) \n",
    "final_uni_bi_dict_set = list(final_uni_bi_dict_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.8 Output File : Vocab File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forming the vocab file by writing the vocab list formed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-7.8  ############################################################\n",
    "\n",
    "# now we can write the vocab list into the vocab file\n",
    "# our vocan list is final_uni_bi_dict_set\n",
    "from nltk.probability import *\n",
    "with open('30154987_vocab.txt', 'w', encoding='utf-8') as f:\n",
    "    for index in range(len(final_uni_bi_dict_set)):\n",
    "        # writing the word along with it's index in the file\n",
    "        f.write(final_uni_bi_dict_set[index]+':'+ str(index)+\"\\n\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.9 Output File : countVec file using CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For getting the countVec file we need countVectorizer to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################   STEP-7.9  ############################################################\n",
    "\n",
    "# Initializing the vectorizer\n",
    "vectorizer = CountVectorizer(analyzer=\"word\")\n",
    "\n",
    "# fitting the final corpus into the the vectorizer\n",
    "data_features = vectorizer1.fit_transform([' '.join(value) for value in final_uni_bi_dict2.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output using sparse matrix\n",
    "with open('30154987_countVec.txt', 'w', encoding='utf-8') as f:\n",
    "    dict_index = 0\n",
    "    for i in final_uni_bi_dict2.keys():\n",
    "        start = i\n",
    "        for word, count in zip(final_uni_bi_dict_set, data_features1.toarray()[dict_index]):\n",
    "            if count > 0:\n",
    "                start = start +\",\"+str(final_uni_bi_dict_set.index(word))+\":\"+str(count)\n",
    "        dict_index = dict_index + 1\n",
    "        f.write(start+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps performed led to a vocabulary that was perfect to be used for further analysis. And the formation of new words that people use.\n",
    "By not removing highly occurring words it's impossible to find new words that are being by humans. It's important to explore new and coming words at the earliest so that  we can expand our knowledge in the domain of language processing and get new information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tutorials form Week 1 to 6. (FIT5196)\n",
    "- Lecture resources form week 1 to 6.(FIT5196)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
